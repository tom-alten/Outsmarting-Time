{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "timegpt = pd.read_parquet('/Users/tomaltenborg/Documents/Master/Master thesis/Notebooks/TimeGPT/Year/M3_yearly_simple_forecasts.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronos = pd.read_parquet('/Users/tomaltenborg/Documents/Master/Master thesis/Notebooks/Chronos/Chronos Year/M3_Chronos_year_forecast.parquet')\n",
    "chronos.drop(columns = ['Low', 'High'], inplace = True)\n",
    "chronos.rename(columns = {'Series_ID' : 'Series', 'Median' : 'Chronos'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "moirai = pd.read_parquet('/Users/tomaltenborg/Documents/Master/Master thesis/Notebooks/MOIRAI/MOIRAI Year/M3_moirai_year_forecasts.parquet')\n",
    "moirai['Series'] = moirai['Series'].astype(int)\n",
    "moirai['Date2'] = chronos['Date']\n",
    "\n",
    "moirai.drop(columns = ['Date'], inplace = True)\n",
    "\n",
    "moirai.rename(columns = {'Date2' : 'Date','Series_ID' : 'Series', 'Median_Forecast' : 'Moirai'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create common dataframe for eaiser analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly = pd.merge(timegpt, chronos, on = ['Series', 'Date'], how='left')\n",
    "yearly = pd.merge(yearly, moirai, on = ['Series', 'Date'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quarterly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "timegptQ = pd.read_parquet('/Users/tomaltenborg/Documents/Master/Master thesis/Notebooks/TimeGPT/Quarter/M3_quarter_simple_forecasts.parquet')\n",
    "timegptQ.reset_index(inplace = True)\n",
    "timegptQ.drop(columns = ['index'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronosQ = pd.read_parquet('/Users/tomaltenborg/Documents/Master/Master thesis/Notebooks/Chronos/Chronos Quarter/M3_Chronos_quarter_forecast.parquet')\n",
    "chronosQ['Date2'] = timegptQ['Date']\n",
    "\n",
    "chronosQ.drop(columns = ['Low', 'High', 'Date'], inplace = True)\n",
    "chronosQ.rename(columns = {'Date2' : 'Date','Series_ID' : 'Series', 'Median' : 'Chronos'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "moiraiQ = pd.read_parquet('/Users/tomaltenborg/Documents/Master/Master thesis/Notebooks/MOIRAI/MOIRAI Quarter/M3_moirai_quarter_forecasts.parquet')\n",
    "moiraiQ['Series'] = moiraiQ['Series'].astype(int)\n",
    "\n",
    "moiraiQ['Date2'] = timegptQ['Date']\n",
    "moiraiQ.drop(columns = ['Date'], inplace = True)\n",
    "moiraiQ.rename(columns = {'Date2' : 'Date','Series_ID' : 'Series', 'Median_Forecast' : 'Moirai'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create common dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly = pd.merge(timegptQ, chronosQ, on = ['Series', 'Date'], how='left')\n",
    "quarterly = pd.merge(quarterly, moiraiQ, on = ['Series', 'Date'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "timegptM = pd.read_parquet('/Users/tomaltenborg/Documents/Master/Master thesis/Notebooks/TimeGPT/Month/M3_month_simple_forecasts.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronosM = pd.read_parquet('/Users/tomaltenborg/Documents/Master/Master thesis/Notebooks/Chronos/Chronos Month/M3_Chronos_month_forecast.parquet')\n",
    "chronosM.drop(columns = ['Low', 'High'], inplace = True)\n",
    "chronosM.rename(columns = {'Series_ID' : 'Series', 'Median' : 'Chronos'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "moiraiM = pd.read_parquet('/Users/tomaltenborg/Documents/Master/Master thesis/Notebooks/MOIRAI/MOIRAI Month/M3_moirai_month_forecasts.parquet')\n",
    "moiraiM['Series'] = moiraiM['Series'].astype(int)\n",
    "moiraiM.rename(columns = {'Median_Forecast' : 'Moirai'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create common dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly = pd.merge(timegptM, chronosM, on = ['Series', 'Date'], how='left')\n",
    "monthly = pd.merge(monthly, moiraiM, on = ['Series', 'Date'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat([yearly, quarterly, monthly])\n",
    "# Sort the concatenated dataframe by series and then date\n",
    "sorted_df = concatenated_df.sort_values(['Series', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add additional information for easier eval\n",
    "Add Na√Øve 2, the deseasonalized random walk, and actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_sheets(excel_path):\n",
    "    \"\"\"\n",
    "    Reads all sheets from an Excel file and combines them into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - excel_path (str): The path to the Excel file.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the combined data from all sheets.\n",
    "    \"\"\"\n",
    "    # Use pandas to read the Excel file and get sheet names\n",
    "    xls = pd.ExcelFile(excel_path)\n",
    "    \n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterate over all sheet names\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        # Read the current sheet into a DataFrame\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=None)\n",
    "        \n",
    "        # Optionally, add a column to indicate the source sheet, if needed\n",
    "        df['Sheet'] = sheet_name\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list into one\n",
    "    combined_yearly = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return combined_yearly\n",
    "\n",
    "excel_path = '/Users/tomaltenborg/Documents/Master/Master thesis/Notebooks/M3 Data/M3Forecast.xls'\n",
    "combined_yearly = read_all_sheets(excel_path)\n",
    "# Rename columns\n",
    "combined_yearly.rename(columns = {0 : 'Series', 1 : 'NF'}, inplace = True)\n",
    "\n",
    "# Change series to int in the same fashion as the other dataframes\n",
    "combined_yearly['Series'] = combined_yearly['Series'].str.replace('N', '')\n",
    "combined_yearly['Series'] = combined_yearly['Series'].str.replace(' ', '')\n",
    "combined_yearly['Series'] = combined_yearly['Series'].astype(int)\n",
    "# Only yearly data for this evaluation\n",
    "combined_yearly = combined_yearly.loc[combined_yearly['Series'] <= 645].copy()\n",
    "# Drop not necessary columns\n",
    "cols_to_drop = list(range(8, 19+1))\n",
    "combined_yearly.drop(columns = cols_to_drop, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change position of sheet column\n",
    "cols = combined_yearly.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "combined_df = combined_yearly[cols]\n",
    "\n",
    "# Rename columns so that 1 is the first forecast period\n",
    "combined_yearly.rename(columns=lambda x: x - 1 if isinstance(x, int) else x, inplace=True)\n",
    "\n",
    "# Melt the dataframe i.e. unpivot\n",
    "df_long_year = pd.melt(combined_yearly, id_vars=['Sheet', 'NF' , 'Series'], value_name='value', var_name='ForecastPeriod')   \n",
    "\n",
    "# Sort the dataframe\n",
    "df_long_year = df_long_year.sort_values(by=['Sheet', 'Series', 'ForecastPeriod']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_sheets(excel_path):\n",
    "    \"\"\"\n",
    "    Reads all sheets from an Excel file and combines them into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - excel_path (str): The path to the Excel file.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the combined data from all sheets.\n",
    "    \"\"\"\n",
    "    # Use pandas to read the Excel file and get sheet names\n",
    "    xls = pd.ExcelFile(excel_path)\n",
    "    \n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterate over all sheet names\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        # Read the current sheet into a DataFrame\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=None)\n",
    "        \n",
    "        # Optionally, add a column to indicate the source sheet, if needed\n",
    "        df['Sheet'] = sheet_name\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list into one\n",
    "    combined_df_quarter = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return combined_df_quarter\n",
    "\n",
    "excel_path = '/Users/tomaltenborg/Documents/Master/Master thesis/Notebooks/M3 Data/M3Forecast.xls'\n",
    "combined_df_quarter = read_all_sheets(excel_path)\n",
    "# Rename columns\n",
    "combined_df_quarter.rename(columns = {0 : 'Series', 1 : 'NF'}, inplace = True)\n",
    "\n",
    "# Change series to int in the same fashion as the other dataframes\n",
    "combined_df_quarter['Series'] = combined_df_quarter['Series'].str.replace('N', '')\n",
    "combined_df_quarter['Series'] = combined_df_quarter['Series'].str.replace(' ', '')\n",
    "combined_df_quarter['Series'] = combined_df_quarter['Series'].astype(int)\n",
    "# Only yearly data for this evaluation\n",
    "combined_df_quarter = combined_df_quarter.loc[(combined_df_quarter['Series'] >= 646) & (combined_df_quarter['Series'] <= 1401)].copy()\n",
    "# Drop not necessary columns\n",
    "combined_df_quarter\n",
    "cols_to_drop = list(range(10, 19+1))\n",
    "combined_df_quarter.drop(columns = cols_to_drop, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change position of sheet column\n",
    "cols = combined_df_quarter.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "combined_df_quarter = combined_df_quarter[cols]\n",
    "\n",
    "# Rename columns so that 1 is the first forecast period\n",
    "combined_df_quarter.rename(columns=lambda x: x - 1 if isinstance(x, int) else x, inplace=True)\n",
    "\n",
    "# Melt the dataframe i.e. unpivot\n",
    "df_long_quarter = pd.melt(combined_df_quarter, id_vars=['Sheet', 'NF' , 'Series'], value_name='value', var_name='ForecastPeriod')   \n",
    "\n",
    "# Sort the dataframe\n",
    "df_long_quarter = df_long_quarter.sort_values(by=['Sheet', 'Series', 'ForecastPeriod']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_sheets(excel_path):\n",
    "    \"\"\"\n",
    "    Reads all sheets from an Excel file and combines them into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - excel_path (str): The path to the Excel file.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the combined data from all sheets.\n",
    "    \"\"\"\n",
    "    # Use pandas to read the Excel file and get sheet names\n",
    "    xls = pd.ExcelFile(excel_path)\n",
    "    \n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterate over all sheet names\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        # Read the current sheet into a DataFrame\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=None)\n",
    "        \n",
    "        # Optionally, add a column to indicate the source sheet, if needed\n",
    "        df['Sheet'] = sheet_name\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list into one\n",
    "    combined_df_month = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return combined_df_month\n",
    "\n",
    "excel_path = '/Users/tomaltenborg/Documents/Master/Master thesis/Notebooks/M3 Data/M3Forecast.xls'\n",
    "combined_df_month = read_all_sheets(excel_path)\n",
    "# Rename columns\n",
    "combined_df_month.rename(columns = {0 : 'Series', 1 : 'NF'}, inplace = True)\n",
    "\n",
    "# Change series to int in the same fashion as the other dataframes\n",
    "combined_df_month['Series'] = combined_df_month['Series'].str.replace('N', '')\n",
    "combined_df_month['Series'] = combined_df_month['Series'].str.replace(' ', '')\n",
    "combined_df_month['Series'] = combined_df_month['Series'].astype(int)\n",
    "\n",
    "combined_df_month = combined_df_month.loc[(combined_df_month['Series'] >= 1402) & (combined_df_month['Series'] <= 2829)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change position of sheet column\n",
    "cols = combined_df_month.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "combined_df_month = combined_df_month[cols]\n",
    "\n",
    "# Rename columns so that 1 is the first forecast period\n",
    "combined_df_month.rename(columns=lambda x: x - 1 if isinstance(x, int) else x, inplace=True)\n",
    "\n",
    "# Melt the dataframe i.e. unpivot\n",
    "df_long_month = pd.melt(combined_df_month, id_vars=['Sheet', 'NF' , 'Series'], value_name='value', var_name='ForecastPeriod')   \n",
    "\n",
    "# Sort the dataframe\n",
    "df_long_month = df_long_month.sort_values(by=['Sheet', 'Series', 'ForecastPeriod']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change format to add as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pivot_table to transform the DataFrame\n",
    "pivot_df_year = df_long_year.pivot_table(index=['Series', 'ForecastPeriod'],  # Adjust or expand index list based on needs\n",
    "                          columns='Sheet',                     # Values in 'Sheet' become new column headers\n",
    "                          values='value',                      # Fill new columns with values from 'value'\n",
    "                          aggfunc='sum')                       # Aggregate function in case of duplicates\n",
    "\n",
    "# Reset the index if you want 'Series' and 'ForecastPeriod' back as columns (optional)\n",
    "pivot_df_year.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pivot_table to transform the DataFrame\n",
    "pivot_df_quarter = df_long_quarter.pivot_table(index=['Series', 'ForecastPeriod'],  # Adjust or expand index list based on needs\n",
    "                          columns='Sheet',                     # Values in 'Sheet' become new column headers\n",
    "                          values='value',                      # Fill new columns with values from 'value'\n",
    "                          aggfunc='sum')                       # Aggregate function in case of duplicates\n",
    "\n",
    "# Reset the index if you want 'Series' and 'ForecastPeriod' back as columns (optional)\n",
    "pivot_df_quarter.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pivot_table to transform the DataFrame\n",
    "pivot_df_month = df_long_month.pivot_table(index=['Series', 'ForecastPeriod'],  # Adjust or expand index list based on needs\n",
    "                          columns='Sheet',                     # Values in 'Sheet' become new column headers\n",
    "                          values='value',                      # Fill new columns with values from 'value'\n",
    "                          aggfunc='sum')                       # Aggregate function in case of duplicates\n",
    "\n",
    "# Reset the index if you want 'Series' and 'ForecastPeriod' back as columns (optional)\n",
    "pivot_df_month.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df_year['Date'] = yearly['Date']\n",
    "pivot_df_quarter['Date'] = quarterly['Date']\n",
    "pivot_df_month['Date'] = monthly['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_methods = pd.concat([pivot_df_year, pivot_df_quarter, pivot_df_month])\n",
    "previous_methods.drop(columns = ['AAM1', 'AAM2'], inplace = True) # These columns not used for yearly, exclude for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Merge for Complete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.merge(sorted_df, previous_methods, on = ['Series', 'Date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.to_parquet('complete_forecast_data.parquet', engine='pyarrow', compression='snappy', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
